{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd5f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: hi!\\nAI: How are you?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation buffer memory - 모든 대화를 기억\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"hi!\"}, {f\"output\": \"How are you?\"})\n",
    "memory.load_memory_variables({})['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7c38928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3')]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation buffer window memory - 최근 대화에 집중.\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "\treturn_messages=True,\n",
    "\tk=2\n",
    ")\n",
    "\n",
    "memory.save_context({\"input\": \"1\"}, {\"output\": \"1\"})\n",
    "memory.save_context({\"input\": \"2\"}, {\"output\": \"2\"})\n",
    "memory.save_context({\"input\": \"3\"}, {\"output\": \"3\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e67de3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human asks the AI to respond in Korean. The AI acknowledges and agrees to respond in Korean, greeting the human as the CEO of a small company in Seongnam, South Korea. The AI responds politely in Korean, addressing the human as the CEO and agrees to look for more ideas for software development using AI.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation suammry memory - 메모리 활용을 위해 llm 을 사용하는 메모리.\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "memory.save_context({\"input\": \"모든 응답은 한국어로 작성해줘.\"}, {\"output\": \"네, 알겠습니다.\"})\n",
    "memory.save_context({\"input\": \"반가워. 나는 대한민국 성남시에서 작은 회사를 운영하는 이 대표라고 해.\"}, {\"output\": \"반갑습니다. 이 대표님\"})\n",
    "memory.save_context({\"input\": \"AI 를 활용한 소프트웨어 개발에 대해 좋은 아이디어가 없을까?\"}, {\"output\": \"더 많은 아이디어를 찾아보겠습니다.\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb72ac0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human greets the AI in Korean and introduces themselves as the CEO of a small company in Seongnam, South Korea. The AI responds in Korean and the human asks if there are any good ideas for software development using AI.'),\n",
       "  AIMessage(content='더 많은 아이디어를 찾아보겠습니다.')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation summary buffer memory - 최근 대화에 집중하면서도, 이전 대화를 잊어버리는 것이 아니라 요약해서 기억함. (buffer memory + summary memory)\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=50, return_messages=True)\n",
    "memory.save_context({\"input\": \"반가워. 나는 대한민국 성남시에서 작은 회사를 운영하는 이 대표라고 해.\"}, {\"output\": \"반갑습니다. 이 대표님\"})\n",
    "memory.save_context({\"input\": \"AI 를 활용한 소프트웨어 개발에 대해 좋은 아이디어가 없을까?\"}, {\"output\": \"더 많은 아이디어를 찾아보겠습니다.\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29d2d146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content=\"The human greets the AI in Korean and introduces themselves as the CEO of a small company in Seongnam, South Korea. The AI responds in Korean and the human asks if there are any good ideas for software development using AI. The AI responds that it will look for more ideas to provide and compliments the human's work in website development for clients. Mr. Lee from Seongnam, South Korea, shares that he is involved in software development and recently worked on creating websites for clients. The AI acknowledges Mr. Lee's work and expresses interest in collaborating on AI software development ideas. Mr. Lee expresses his pleasure in meeting the AI and introduces himself as the CEO of a small company in Seongnam, South Korea, mentioning his recent work in website development for clients. The AI compliments Mr. Lee's work and expresses interest in collaborating on AI software development ideas.\"),\n",
       "  AIMessage(content='더 많은 아이디어를 찾아보겠습니다.')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conversation knowledge graph memory - 요약해서 엔티티를 추출하는 메모리\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "memroy = ConversationKGMemory(llm=llm, return_messages=True)\n",
    "memory.save_context({\"input\": \"반가워. 나는 대한민국 성남시에서 작은 회사를 운영하는 Mr.Lee라고 해.\"}, {\"output\": \"반갑습니다. Mr.Lee\"})\n",
    "memory.save_context({\"input\": \"나는 소프트웨어 개발 및 공급을 하고 있어. 최근에는 클라이언트의 홈페이지를 제작했어.\"}, {\"output\": \"멋진 일을 하고 계시네요.\"})\n",
    "memory.save_context({\"input\": \"AI 를 활용한 소프트웨어 개발에 대해 좋은 아이디어가 없을까?\"}, {\"output\": \"더 많은 아이디어를 찾아보겠습니다.\"})\n",
    "memory.load_memory_variables({\"inputs\": \"Mr.Lee의 최근 작업에 대해서 알려줘.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc11d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# off the shelf chain - 즉시 꺼내쓸 수 있도록 미리 만들어 놓은 체인(?)\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "\tllm=llm,\n",
    "\tmax_token_limit=50,\n",
    "\tmemory_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "# 그냥 질문하면 기억을 못하니, 메모리도 함께 템플릿으로 던진다. memory_key 는 맞춰야 한다.\n",
    "template = \"\"\"\n",
    "\t당신은 대화를 통해 사람에게 도움을 주는 AI 입니다.\n",
    "\n",
    "\t{chat_history}\n",
    "\tHuman: {question}\n",
    "\tYou: \n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "\tllm=llm,\n",
    "\tmemory=memory,\n",
    "\t# prompt=PromptTemplate.from_template(\"{question}\"),\n",
    "\tprompt=PromptTemplate.from_template(template),\n",
    "\tverbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f7f0754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "\t당신은 대화를 통해 사람에게 도움을 주는 AI 입니다.\n",
      "\n",
      "\tHuman: Hello, my name is Mr.Lee\n",
      "AI: Hello Mr. Lee! How can I assist you today?\n",
      "\tHuman: what is my name?\n",
      "\tYou: \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Mr. Lee.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# off the shelf chain - 즉시 꺼내쓸 수 있도록 미리 만들어 놓은 체인 (계속)\n",
    "# chain.predict(question=\"Hello, my name is Mr.Lee\")\n",
    "chain.predict(question=\"what is my name?\")\n",
    "# memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26c988c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat based memory \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "\tllm=llm,\n",
    "\tmax_token_limit=50,\n",
    "\tmemory_key=\"chat_history\",\n",
    "\treturn_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "\t(\"system\", \"당신은 대화를 통해 사람에게 도움을 주는 AI 입니다.\"),\n",
    "\tMessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "\t(\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "\tllm=llm,\n",
    "\tmemory=memory,\n",
    "\tprompt=prompt,\n",
    "\tverbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53191489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: 당신은 대화를 통해 사람에게 도움을 주는 AI 입니다.\n",
      "Human: Hello, my name is Mr.Lee\n",
      "AI: Hello Mr. Lee! How can I assist you today?\n",
      "Human: what is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Mr. Lee. How can I assist you further, Mr. Lee?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chat based memory (계속)\n",
    "# chain.predict(question=\"Hello, my name is Mr.Lee\")\n",
    "chain.predict(question=\"what is my name?\")\n",
    "# memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9636dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL based memory\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "\tllm=llm,\n",
    "\tmax_token_limit=50,\n",
    "\treturn_messages=True,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "\t(\"system\", \"당신은 대화를 통해 사람에게 도움을 주는 AI 입니다.\"),\n",
    "\tMessagesPlaceholder(variable_name=\"history\"),\n",
    "\t(\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "def load_memory(_):\n",
    "\tprint(memory.load_memory_variables({})[\"history\"])\n",
    "\treturn memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | llm\n",
    "\n",
    "def invoke_chain(question):\n",
    "\tresult = chain.invoke({\"question\": question})\n",
    "\tmemory.save_context(\n",
    "\t\t{\"input\": question},\n",
    "\t\t{\"output\": result.content},\n",
    "\t)\n",
    "\tprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b42edbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='my favorite food is pizza.'), AIMessage(content='Pizza is a popular choice! What do you like most about pizza? The cheese, the toppings, or the crust?')]\n",
      "content=\"Your favorite food is pizza! It's great that you enjoy it so much. Do you have a favorite type of pizza or favorite toppings?\"\n"
     ]
    }
   ],
   "source": [
    "# LCEL based memory (계속)\n",
    "\n",
    "# invoke_chain(\"my name is Mr. Lee\")\n",
    "# invoke_chain(\"what is my name?\")\n",
    "# invoke_chain(\"my favorite food is pizza.\")\n",
    "invoke_chain(\"what is my favorite food?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
